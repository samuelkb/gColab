{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We have learned how CNNs will take an input image, and through a series of layers, transform that input into an output that's much smaller in the x, y dimensions, but much greater in depth. Along the way, the CNN is discarding spatial information from the input image and isolating high level information about its content. Some of this structure can also be thought of as a kind of data compression; compressing from an image into something like a feature vector, which is basically a feature map produced after an input has gone through a series of layers squished into a vector shape. \n",
    "\n",
    "This is part fo what makes up something called an autoencoder, which is what we will learn about this lesson. An autoencoder has two main components: an encoder that compresses some input data, and a decoder that reconstructs data form the compressed representation. Why is htis kind of structure even useful? \n",
    "\n",
    "It ends up being useful in a number of cases. Autoencoders are used in a traditional data compression sense, in that they can learn to reduce the dimensionality of any input. Then, anyone can use a compressed representation to share it, or view it and so on, faster than they could with the original input. We might think of something like a jpg or mp3 file type, which contain explicit rules for compressing images and audio. The difference is that an autoencoder learns efficient data compression and decompression functions instead of having them designed, encoded by a human. \n",
    "\n",
    "Autoencoders have shown the most promise in image denoising techniques and in filling in missing data. This structure will also come up again as we learn about generative models that can take in an image and transform it into a related space such as form gray scale to color or from low to high resolution images. \n",
    "\n",
    "The encoder and decodeer are both built with neural networks. Generally, the whole network is trained by minimizing the difference between the input and the output. In that way, the middle layer will be a compressed representation of the input data from which we can reconstruct the original data. \n",
    "\n",
    "<img src=\"assets/Autoencoders.png\">\n",
    "\n",
    "The key aspect of an autoencoder is its ability to compress an image such that its content is still maintained. Then later, we may be able to use this compressed representation to generate something else. We will show how to build autoencoders in PyTorch. We will start with a simple example where we will compress images. Then, since this is ImageData, we will improve it by using convolutional layers "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
