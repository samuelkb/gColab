{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We have learned how CNNs will take an input image, and through a series of layers, transform that input into an output that's much smaller in the x, y dimensions, but much greater in depth. Along the way, the CNN is discarding spatial information from the input image and isolating high level information about its content. Some of this structure can also be thought of as a kind of data compression; compressing from an image into something like a feature vector, which is basically a feature map produced after an input has gone through a series of layers squished into a vector shape. \n",
    "\n",
    "This is part fo what makes up something called an autoencoder, which is what we will learn about this lesson. An autoencoder has two main components: an encoder that compresses some input data, and a decoder that reconstructs data form the compressed representation. Why is htis kind of structure even useful? \n",
    "\n",
    "It ends up being useful in a number of cases. Autoencoders are used in a traditional data compression sense, in that they can learn to reduce the dimensionality of any input. Then, anyone can use a compressed representation to share it, or view it and so on, faster than they could with the original input. We might think of something like a jpg or mp3 file type, which contain explicit rules for compressing images and audio. The difference is that an autoencoder learns efficient data compression and decompression functions instead of having them designed, encoded by a human. \n",
    "\n",
    "Autoencoders have shown the most promise in image denoising techniques and in filling in missing data. This structure will also come up again as we learn about generative models that can take in an image and transform it into a related space such as form gray scale to color or from low to high resolution images. \n",
    "\n",
    "The encoder and decodeer are both built with neural networks. Generally, the whole network is trained by minimizing the difference between the input and the output. In that way, the middle layer will be a compressed representation of the input data from which we can reconstruct the original data. \n",
    "\n",
    "<img src=\"assets/Autoencoders.png\">\n",
    "\n",
    "The key aspect of an autoencoder is its ability to compress an image such that its content is still maintained. Then later, we may be able to use this compressed representation to generate something else. We will show how to build autoencoders in PyTorch. We will start with a simple example where we will compress images. Then, since this is ImageData, we will improve it by using convolutional layers.\n",
    "\n",
    "So, let's be defining and training an autoencoder!\n",
    "\n",
    "# A Simple Autoencoder\n",
    "\n",
    "We'll start off by building a simple autoencoder to compress the MNIST dataset, which has images of 28x28x1 array with a total number of 784 pixels. With autoencoders, we pass input data through an encoder that makes a compressed representation of the input. Then, this representation is passed through a decoder to reconstruct the input data. Generally the encoder and decoder will be built with neural networks, then trained on example data.\n",
    "\n",
    "<img src='assets/autoencoder_1.png' />\n",
    "\n",
    "### Compressed Representation\n",
    "\n",
    "A compressed representation can be great for saving and sharing any kind of data in a way that is more efficient than storing raw data. Compressed data is often cheaper to store in data centers and faster to share across Wi-Fi connected devices. In practice, the compressed representation often holds key information about an input image and we can use it for denoising images or oher kinds of reconstruction and transformation!\n",
    "\n",
    "<img src='assets/denoising.png' width=60%/>\n",
    "\n",
    "\n",
    "The idea is that we define our encoder and decoder as neural networks. Then, we train this complete autoencoder by passing in an original image and getting back as output a reconstructed image. We can then compare the original with the reconstruction. We want the original and reconstructed image to be as close as possible. So, our loss will actually be comparing these pixel values and measuring the difference between the original and reconstructed images. Once this whole network is trained, we should have a working encoder and decoder portion of a network, and we will be able to use either part to either compress or decompress a certain image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only defining train and test loaders here, and that is because in this case, we really just want to get our training loss as low as possible. This is not a typical classification task, and validation sets are really most useful when we are trying to predict a quantity like a class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test dataloaders\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x121c0a750>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEvCAYAAAAtufaDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPk0lEQVR4nO3db4xVdX7H8c+nqA9EFMhWJKyW1RgsGjs2iI2aqjGsf6LRUbdZEjc0GvGBJJhsSA1PVh9gSFW2IRoDG3HR7LJu4lrRNFUjKG1siAOiItRqDOuCE4gigvgvMN8+mGMy4Aznx7135swX3q+E3Ht/8+V3v8fDfDzn3N+ccUQIALL6q6YbAIB2EGIAUiPEAKRGiAFIjRADkBohBiC1E0byzWyzngNAqz6NiL8+fLCtIzHb19p+3/aHtu9rZy4AqPHnwQZbDjHbYyQ9Juk6SdMlzbY9vdX5AKAV7RyJzZT0YUR8FBHfSfqDpJs60xYAlGknxKZI+suA19urMQAYMe1c2PcgYz+4cG97rqS5bbwPAAypnRDbLunMAa9/LOmTw4siYrmk5RKfTgLovHZOJ9+UdK7tn9g+SdLPJa3uTFsAUKblI7GIOGB7nqSXJI2RtCIi3utYZwBQwCN5PzFOJwG0YUNEzDh8kB87ApAaIQYgNUIMQGqEGIDUCDEAqRFiAFIjxACkRogBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFIjxACkRogBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFIjxACkRogBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFIjxACkRogBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFIjxACkdkLTDSC3MWPG1NacdtppI9DJoebNm1dUd/LJJxfVTZs2rajunnvuqa15+OGHi+aaPXt2Ud0333xTW7N48eKiuR544IGiutGkrRCzvU3SPkkHJR2IiBmdaAoASnXiSOyqiPi0A/MAwFHjmhiA1NoNsZD0su0NtucOVmB7ru0e2z1tvhcA/EC7p5OXRcQntk+X9Irt/42IdQMLImK5pOWSZDvafD8AOERbR2IR8Un1uEvSc5JmdqIpACjVcojZHmt73PfPJf1U0uZONQYAJdo5nZwk6Tnb38/z+4j4z450BQCFWg6xiPhI0t91sBcM4ayzzqqtOemkk4rmuvTSS4vqLr/88qK68ePH19bceuutRXONZtu3by+qW7p0aW1Nd3d30Vz79u0rqnv77bdra15//fWiuTJiiQWA1AgxAKkRYgBSI8QApEaIAUiNEAOQGiEGIDVCDEBqhBiA1BwxcjeW4C4Wh+rq6iqqW7NmTW1NE7eAPhb09fUV1d1xxx1FdV9++WU77Ryit7e3qO7zzz+vrXn//ffbbWc02DDY3aM5EgOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSI8QApEaIAUiNEAOQWru/dxJt+Pjjj4vqPvvss9qaY2HF/vr164vq9uzZU1tz1VVXFc313XffFdU9/fTTRXUYeRyJAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSI8QApMZi1wbt3r27qG7BggW1NTfccEPRXG+99VZR3dKlS4vqSmzatKmobtasWUV1+/fvr605//zzi+aaP39+UR1GL47EAKRGiAFIjRADkBohBiA1QgxAaoQYgNQIMQCpEWIAUiPEAKTmiBi5N7NH7s2OM6eeempR3b59+4rqli1bVlR355131tbcfvvtRXOtWrWqqA7HrQ0RMePwwdojMdsrbO+yvXnA2ETbr9j+oHqc0OluAaBEyenkbyVde9jYfZJejYhzJb1avQaAEVcbYhGxTtLhP6l8k6SV1fOVkm7ubFsAUKbVC/uTIqJXkqrH0zvXEgCUG/Zb8dieK2nucL8PgONTq0diO21PlqTqcddQhRGxPCJmDPapAgC0q9UQWy1pTvV8jqTnO9MOABydkiUWqyT9j6RptrfbvlPSYkmzbH8gaVb1GgBGXO01sYiYPcSXru5wLwBw1LjH/jFi7969HZ3viy++6Nhcd911V1HdM888U1TX19fXTjs4xvCzkwBSI8QApEaIAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBS4x77GNTYsWOL6l544YXamiuuuKJoruuuu66o7uWXXy6qwzGntXvsA8BoRogBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFJjsSvacs4559TWbNy4sWiuPXv2FNWtXbu2tqanp6dorscee6yobiS/TzAkFrsCOPYQYgBSI8QApEaIAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKmxYh/Drru7u6juySefLKobN25cO+0cYuHChUV1Tz31VFFdb29vO+3gyFixD+DYQ4gBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqRFiAFIjxACkxop9jBoXXHBBUd2SJUtqa66++up22znEsmXLiuoWLVpUW7Njx4522zletbZi3/YK27tsbx4wdr/tHbY3VX+u73S3AFCi5HTyt5KuHWT81xHRVf35j862BQBlakMsItZJ2j0CvQDAUWvnwv482+9Up5sThiqyPdd2j+2yXwQIAEeh1RB7XNI5krok9Up6ZKjCiFgeETMGuyAHAO1qKcQiYmdEHIyIPkm/kTSzs20BQJmWQsz25AEvuyVtHqoWAIbTCXUFtldJulLSj2xvl/QrSVfa7pIUkrZJunv4WgSAobHYFemMHz++tubGG28smqv0lti2i+rWrFlTWzNr1qyiufAD3J4awLGHEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSI8QApEaIAUiNFfs4rn377bdFdSecUPsTepKkAwcO1NZcc801RXO99tprRXXHEVbsAzj2EGIAUiPEAKRGiAFIjRADkBohBiA1QgxAaoQYgNQIMQCplS1DBkbAhRdeWFR322231dZcfPHFRXOVrsQvtWXLltqadevWdfQ9j3cciQFIjRADkBohBiA1QgxAaoQYgNQIMQCpEWIAUiPEAKRGiAFIjRX7aMu0adNqa+bNm1c01y233FJUd8YZZxTVddLBgweL6np7e2tr+vr62m0HA3AkBiA1QgxAaoQYgNQIMQCpEWIAUiPEAKRGiAFIjRADkBqLXY8zpQtFZ8+eXVRXspB16tSpRXM1oaenp6hu0aJFRXWrV69upx20oPZIzPaZttfa3mr7Pdvzq/GJtl+x/UH1OGH42wWAQ5WcTh6Q9MuI+FtJ/yDpHtvTJd0n6dWIOFfSq9VrABhRtSEWEb0RsbF6vk/SVklTJN0kaWVVtlLSzcPUIwAM6agu7NueKukiSeslTYqIXqk/6CSd3vHuAKBG8YV926dIelbSvRGx13bp35sraW5r7QHAkRUdidk+Uf0B9ruI+FM1vNP25OrrkyXtGuzvRsTyiJgRETM60TAADFTy6aQlPSFpa0QsGfCl1ZLmVM/nSHq+8+0BwJGVnE5eJukXkt61vakaWyhpsaQ/2r5T0seSfjYsHQLAEdSGWET8t6ShLoBd3dl2AODosGI/gUmTJtXWTJ8+vWiuRx99tKjuvPPOK6prwvr162trHnrooaK5nn++7CoIt5QevfjZSQCpEWIAUiPEAKRGiAFIjRADkBohBiA1QgxAaoQYgNQIMQCpsWJ/GEycOLGobtmyZUV1XV1dtTVnn3120VxNeOONN4rqHnnkkaK6l156qbbm66+/LpoL+XEkBiA1QgxAaoQYgNQIMQCpEWIAUiPEAKRGiAFIjRADkBqLXSuXXHJJUd2CBQtqa2bOnFk015QpU4rqmvDVV18V1S1durS25sEHHyyaa//+/UV1wEAciQFIjRADkBohBiA1QgxAaoQYgNQIMQCpEWIAUiPEAKRGiAFIjRX7le7u7o7WddKWLVtqa1588cWiuQ4cOFBUV3qr6D179hTVAcOFIzEAqRFiAFIjxACkRogBSI0QA5AaIQYgNUIMQGqEGIDUCDEAqTkiRu7N7JF7MwDHmg0RMePwwdojMdtn2l5re6vt92zPr8bvt73D9qbqz/XD0TUAHEnJz04ekPTLiNhoe5ykDbZfqb7264h4ePjaA4Ajqw2xiOiV1Fs932d7q6TR+7vGABxXjurCvu2pki6StL4ammf7HdsrbE/odHMAUKc4xGyfIulZSfdGxF5Jj0s6R1KX+o/UBr13i+25tnts97TfLgAcqujTSdsnSnpR0ksRsWSQr0+V9GJEXFAzD59OAmhVy59OWtITkrYODDDbkweUdUva3IkuAeBolHw6eZmkX0h61/amamyhpNm2uySFpG2S7h6G/gDgiFjsCiCL1k4nAWA0I8QApEaIAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSI8QApEaIAUiNEAOQGiEGIDVCDEBqhBiA1AgxAKkRYgBSI8QApEaIAUit5BeFdNKnkv582NiPqvGssvcv5d+G7P1L+bdhJPr/m8EGR/QXhQzagN0z2M3/s8jev5R/G7L3L+Xfhib753QSQGqEGIDURkOILW+6gTZl71/Kvw3Z+5fyb0Nj/Td+TQwA2jEajsQAoGWNhZjta22/b/tD2/c11Uc7bG+z/a7tTbZ7mu6nhO0VtnfZ3jxgbKLtV2x/UD1OaLLHIxmi//tt76j2wybb1zfZ45HYPtP2Wttbbb9ne341nmkfDLUNjeyHRk4nbY+R9H+SZknaLulNSbMjYsuIN9MG29skzYiINOt7bP+jpC8lPRURF1Rj/yppd0Qsrv6HMiEi/qXJPocyRP/3S/oyIh5usrcStidLmhwRG22Pk7RB0s2S/ll59sFQ2/BPamA/NHUkNlPShxHxUUR8J+kPkm5qqJfjSkSsk7T7sOGbJK2snq9U/z/IUWmI/tOIiN6I2Fg93ydpq6QpyrUPhtqGRjQVYlMk/WXA6+1q8D9CG0LSy7Y32J7bdDNtmBQRvVL/P1BJpzfcTyvm2X6nOt0ctadiA9meKukiSeuVdB8ctg1SA/uhqRDzIGMZPya9LCL+XtJ1ku6pTnUw8h6XdI6kLkm9kh5ptJsCtk+R9KykeyNib9P9tGKQbWhkPzQVYtslnTng9Y8lfdJQLy2LiE+qx12SnlP/aXJGO6vrHN9f79jVcD9HJSJ2RsTBiOiT9BuN8v1g+0T1f/P/LiL+VA2n2geDbUNT+6GpEHtT0rm2f2L7JEk/l7S6oV5aYntsdVFTtsdK+qmkzUf+W6PWaklzqudzJD3fYC9H7ftv/kq3RvF+sG1JT0jaGhFLBnwpzT4Yahua2g+NLXatPn79N0ljJK2IiEWNNNIi22er/+hL6r8byO8zbIPtVZKuVP9dB3ZK+pWkf5f0R0lnSfpY0s8iYlRePB+i/yvVfwoTkrZJuvv760ujje3LJf2XpHcl9VXDC9V/TSnLPhhqG2argf3Ain0AqbFiH0BqhBiA1AgxAKkRYgBSI8QApEaIAUiNEAOQGiEGILX/BwIYAbXRjvhNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear Autoencoder\n",
    "\n",
    "We'll train an autoencoder with these images by flattening them into 784 length vectors. The images from this dataset are already normalized such that the values are between 0 and 1. Let's start by building a simple autoencoder. The encoder and decoder should be made of **one linear layer**. The units that connect the encoder and decoder will be the _compressed representation_. The decoder will also be made of one linear layer that will up sample or increase the dimension of the compressed representation. We want this layer to output a vector of length 784. Later, we will reshape this vector output into a 28x28 reconstructed image, and then we will be able to compare these two. \n",
    "\n",
    "Since the images are normalized between 0 and 1, we need to use a **sigmoid activation on the output layer** to get values that match this input value range.\n",
    "\n",
    "<img src='assets/simple_autoencoder.png' width=50% />\n",
    "\n",
    "\n",
    "#### TODO: Build the graph for the autoencoder in the cell below. \n",
    "> The input images will be flattened into 784 length vectors. The targets are the same as the inputs. \n",
    "> The encoder and decoder will be made of two linear layers, each.\n",
    "> The depth dimensions should change as follows: 784 inputs > **encoding_dim** > 784 outputs.\n",
    "> All layers will have ReLu activations applied except for the final output layer, which has a sigmoid activation.\n",
    "\n",
    "**The compressed representation should be a vector with dimension `encoding_dim=32`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (fce1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fce2): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (fcd1): Linear(in_features=32, out_features=256, bias=True)\n",
      "  (fcd2): Linear(in_features=256, out_features=784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        ## encoder ##\n",
    "        self.fce1 = nn.Linear(28 * 28, 256)\n",
    "        self.fce2 = nn.Linear(256, encoding_dim)\n",
    "        ## decoder ##\n",
    "        self.fcd1 = nn.Linear(encoding_dim, 256)\n",
    "        self.fcd2 = nn.Linear(256, 784)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # define feedforward behavior \n",
    "        # and scale the *output* layer with a sigmoid activation function\n",
    "        x = F.relu(self.fce1(x))\n",
    "        x = F.relu(self.fce2(x))\n",
    "        x = F.relu(self.fcd1(x))\n",
    "        x = F.sigmoid(self.fcd2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "encoding_dim = 32\n",
    "model = Autoencoder(encoding_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "Here We'll write a bit of code to train the network. We are not too interested in validation here, so we'll just monitor the training loss and the test loss afterwards. \n",
    "\n",
    "We are not concerned with labels in this case, just images, which we can get from the `train_loader`. Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing _quantities_ rather than probabilistic values. So, in this case, We'll use `MSELoss`. And compare output images and input images as follows:\n",
    "```\n",
    "loss = criterion(outputs, images)\n",
    "```\n",
    "\n",
    "We also use `Adam` optimizer simply because it is been proven to work better in these scenarios.\n",
    "\n",
    "Otherwise, this is pretty straightfoward training with PyTorch. We flatten our images, pass them into the autoencoder, and record the training loss as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code, note we use images, we are actually not interested in any labels only in original images and the reconstructions. Then we flatten these images and then pass those vectors to our model.\n",
    "\n",
    "Our model returns recosntructed image outputs. Then finally, we are comparing the input images and this output reconstructed image. So our loss is actually telling us how good of a reconstructed image our model has produced. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mb78200/opt/anaconda3/envs/my_p3_env/lib/python3.7/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.502579\n",
      "Epoch: 2 \tTraining Loss: 0.303383\n",
      "Epoch: 3 \tTraining Loss: 0.271715\n",
      "Epoch: 4 \tTraining Loss: 0.246890\n",
      "Epoch: 5 \tTraining Loss: 0.232441\n",
      "Epoch: 6 \tTraining Loss: 0.223691\n",
      "Epoch: 7 \tTraining Loss: 0.217513\n",
      "Epoch: 8 \tTraining Loss: 0.212696\n",
      "Epoch: 9 \tTraining Loss: 0.208922\n",
      "Epoch: 10 \tTraining Loss: 0.205831\n",
      "Epoch: 11 \tTraining Loss: 0.203217\n",
      "Epoch: 12 \tTraining Loss: 0.201044\n",
      "Epoch: 13 \tTraining Loss: 0.199142\n",
      "Epoch: 14 \tTraining Loss: 0.197452\n",
      "Epoch: 15 \tTraining Loss: 0.195934\n",
      "Epoch: 16 \tTraining Loss: 0.194628\n",
      "Epoch: 17 \tTraining Loss: 0.193400\n",
      "Epoch: 18 \tTraining Loss: 0.192255\n",
      "Epoch: 19 \tTraining Loss: 0.191229\n",
      "Epoch: 20 \tTraining Loss: 0.190259\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in train_loader:\n",
    "        # _ stands in for labels, here\n",
    "        images, _ = data\n",
    "        # flatten images\n",
    "        images = images.view(images.size(0), -1)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, images)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that the biggest decreases happened at the start of training, and the minor decreases around epoch 13 or so. So, if we train for loger than 20 epochs, we couldn't be sure if we would have seen much of an improvement, but this loss is only one way to check how well our model is doing. \n",
    "\n",
    "For classification model recall, we would often look at classification accuracy, but here the task is a little different, this whole time we are comparing original input images to the recostructions. So, a better way to see how our encoder is doing is to look directly at those reconstructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking out the results\n",
    "\n",
    "Below, We are loading in a batch of test data and we care only about the images. Then, we flattened those test images and pass them to our model to get some reconstructions. Finally, we resize that output, so that is reshaped into a 28x28 image, and that code displays 10 original images and their 10 reconstructions underneath them.\n",
    "\n",
    "We've plotted some of the test images along with their reconstructions. For the most part these look pretty good except for some blurriness in some parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "images_flatten = images.view(images.size(0), -1)\n",
    "# get sample outputs\n",
    "output = model(images_flatten)\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "\n",
    "# output is resized into a batch of images\n",
    "output = output.view(batch_size, 1, 28, 28)\n",
    "# use detach when it's an output that requires_grad\n",
    "output = output.detach().numpy()\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "# input images on top row, reconstructions on bottom\n",
    "for images, row in zip([images, output], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(np.squeeze(img), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "We're dealing with images here, so we can (usually) get better performance using convolution layers. So, next we'll build a better autoencoder with convolutional layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learnable Upsampling\n",
    "\n",
    "Now that we have seen how to build an autoencoder out of linear layers, let's improve on this model using convolutional layers. Convolutional layers give us a way to preserve spatial information. So, generally, they are part of a better and more elegant solution for encoding and decoding images. The encoder portion of an autoencoder is something that we have seen before in our notebook on CNNs. \n",
    "\n",
    "It is typically made of a series of convolutional and maxpooling layers that downsample the spatial dimensions of an input image. So, we know roughly how to get from an input to a compressed representation already. The decoder is something we haven't really seen before. How can we go from a compressed representation to a reconstructed image?\n",
    "\n",
    "In our example above we have seen that we can just define a linear layer to transform any number of inputs into our desired output size, and so we could easily reverse the encoder steps. Here, with an input image instead of a vector, there are a couple of approaches we might take, but the end goal is the same. We want to reverse the downsampling process that happened in the encoder. We want to increase the spatial dimensions of a compressed input to produce a reconstructed image that has the same shape as the original input. \n",
    "\n",
    "So, instead of down sampling using maxpooling, we can imagine trying to upsample an image by unpooling the pixel values in an input. We could use an interpolation technique like nearest neighbors or another kind of linear interpolation. Nearest neighbors expands a given area by copying over a single pixel value from an input image to, say, a 2x2 in the larger output image. \n",
    "\n",
    "<img src=\"assets/nearestNeighbors.png\">\n",
    "\n",
    "If we have ever try to enlarge a low resolution image this upsampling is usually what happens. But interpolation is a fairly crude way to upsample an image. In the case of nearest neighbors, we are just copying over existing values. But a realistic larger image is likely to have more variety in pixel values, and so there may be better way to upsample its image. \n",
    "\n",
    "So, we could also try to learn how to best upsample an image. If we want our network to learn how to upsample, we can use a transpose convolutional layer. This layer does not use a predefined interpolation method, instead it has learnable parameters. We will sometimes hear these referred to as deconvolutional layers but it is not strictly undoing a convolution step. We find it most helpful to think of transpose convolution as a way to upsample existing input values using filter weights, in a way that is similar to traditional convolution. \n",
    "\n",
    "Let's get into the math behind a transpose convolutional layer!\n",
    "\n",
    "# Transpose convolutions\n",
    "\n",
    "Imagine that we have an input image that is 4x4 pixels, small but good for an example. We want to produce an output filtered image that is the same size. This is our usual case, and we have seen that we can use a 3x3 kernel and a padding of one to get this result. Convolution tell us to multiply each pixel value in the input image by the weights of the overlaid kernel, and then add them all up to get the resultant output pixel value. We typically have a stride of one to do this for every pixel in the input image, as shown at next:\n",
    "\n",
    "<img src=\"assets/ConvolutionSimple.png\">\n",
    "\n",
    "But what if we had a stride of two? Well our kernel would move to the right and down by 2x2 pixels at a time instead of one, and the output will be a 2x2 filtered image. The filter is moving two pixels for every one pixel in the output image, and it turns out that the stride value is roughly the radio of the input to output dimensions. In this way, a convolutional operation can actually down sample an image.\n",
    "\n",
    "<img src=\"assets/ConvolutionalStrideTo2.png\">\n",
    "\n",
    "Now, keep this stride value of two in mind as we show how transpose convolution reverses this process. It can take a single pixel from a 2x2 image, place a 3x3 kernel over it, then multiply that one pixel value by one kernel weights to get a resultant 3x3 pixel area. Say we do this whith the first pixel, and we get a 3x3 output area with some center. Then, we try to do the same thing for the second pixel. We will assume that our stride is two in the output, so the center of the generated area will be two to the right of two center of the 3x3 area. \n",
    "\n",
    "<img src=\"assets/CNNtoTransposeStep1.png\">\n",
    "\n",
    "Now, transpose convolution will produce another 3x3 area, one that overlaps with the initial one. In the case of overlap, these one will just be summed together. If we do this for all four of the input pixels, it will give a 5x5 resultant area.\n",
    "\n",
    "<img src=\"assets/CNNtoTransposeStep2.png\">\n",
    "\n",
    "There are some options to add or substract padding from this output, but the most common case, it will use a 2x2 filter and a stride of two to double the x and y dimensions of two input. The weights in the kernel here are learned much like in a convolutional layer. Only this time, the purpose is to learn effective upsampling.\n",
    "\n",
    "<img src=\"assets/CNNTransposeStride.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
