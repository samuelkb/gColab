{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We have learned a lot about CNNs. And in this notebook we will talk about a detail that is been happening behind the scenes when we build and train neural networks in PyTorch, and that's weights initialization.\n",
    "\n",
    "Weight initialization happens right when a model is created, before it sees any training data, and it generally only happens one time when a model is first created. Weight initialization is all about how we can instantiate a model, like a CNN, so that its weights, the model parameters, are starting off with the best initial values for a given task. \n",
    "\n",
    "We recommend to thing of it this way, any neuron in a model has some weights and biases that operate on some input values and transform them into desired outputs. It is how we might transform an input image into some class scores. A model is trying to learn the best weights to map inputs to outputs and, say, most accurately classify some given images. \n",
    "\n",
    "When we did transfer learning, we basically initialized a network with the best pre-trained weights, weights that we know are good values for image classification. But for a non-pretrained model, how should we initialize the weights? We could start for example by saying all the weights are going to be zero, and we will train them until they are shifted up or down, or we could start out with large weight values, or even some random values, and see what happens.\n",
    "\n",
    "These are the questions that we will be exploring in this notebook.We will demostrate the effect of different weight initialization strategiess, discuss how we might decide on initial weights and what indicates good initial behavior during training. \n",
    "\n",
    "The plan: we want to define one kind of model, a simple MLP,  and we will initialize the weights of this model with different values, and compare how the training losses decrease over the first epoch or two of training. Same models, different initial weights. \n",
    "\n",
    "Some initialization strategies may offer some really big improvements to how the loss decreases, and others may offer small improvements. Big or small, we can learn from all of these differences. We will be using the Fashion-MNIST dataset. A simple ataset like this one is often used to test network performance, because the dataset is simple to use and its training behavior is fairly well understood. We know for example that we should be able to get the training loss to reliably decrease. \n",
    "\n",
    "We will be using a `helpers.py` file, the purpose of this notebook is not to train the model until it performs well on the classification task, but rather we just want to see how well the models train given different initial weights. In that `helpers.py` file we have a main helper function `get_loss_acc`, which takes in a model, and the training and validation model. We defined the length fo time to train a model choosing 2 epochs, and we also defined a loss and optimization functions there, using cross-entropy  and an Adam optimizer, but feel free to change this to Stochastic gradient descent and see how it performs. Then in the function, we are actually going to train the model for 2 epochs, and record the training loss as we go. This loss is going to be recorded in a list called `loss_batch`. After a model has trained for 2 epochs, we then see how it performs on our validation data set. We compare the predicted and the correct class, and see which ones match. Finally, we return the list of training loss over time and that validation accuracy. Another helper function will compare its weights, which takes in a list of models. For each model in our list, it will record the training loss and the validation accuracy over 2 epochs, and compare at least two models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
